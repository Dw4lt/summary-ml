\contentsline {section}{\numberline {1}Chapter 2 - Statistical Learning}{4}
\contentsline {subsection}{\numberline {1.1}Overview (What is Statistical Learning)}{4}
\contentsline {paragraph}{General}{4}
\contentsline {paragraph}{Errors}{4}
\contentsline {paragraph}{Inference}{4}
\contentsline {paragraph}{parametric}{4}
\contentsline {paragraph}{non-parametric}{5}
\contentsline {paragraph}{supervised vs. unsupervised}{5}
\contentsline {paragraph}{regression vs. classification}{5}
\contentsline {subsection}{\numberline {1.2}Assessing Model Accuracy}{5}
\contentsline {paragraph}{Quality of fit (fitness)}{5}
\contentsline {paragraph}{bias vs. variance}{5}
\contentsline {subparagraph}{variance}{6}
\contentsline {subparagraph}{bias}{6}
\contentsline {paragraph}{classification}{6}
\contentsline {paragraph}{bayes classifier}{6}
\contentsline {paragraph}{K-nearest neighbors}{6}
\contentsline {section}{\numberline {2}Chapter 3 - Linear Regression}{7}
\contentsline {subsection}{\numberline {2.1}Simple linear regression}{7}
\contentsline {paragraph}{General}{7}
\contentsline {paragraph}{Estimating the coefficients}{7}
\contentsline {paragraph}{Accuracy of the coefficient estimates}{7}
\contentsline {subparagraph}{true variance}{8}
\contentsline {subparagraph}{estimated variance}{9}
\contentsline {subparagraph}{hypothesis}{9}
\contentsline {paragraph}{Accuracy of the model}{9}
\contentsline {subsection}{\numberline {2.2}Multiple linear regression}{10}
\contentsline {paragraph}{Estimating the coefficients}{10}
\contentsline {paragraph}{Is there a relationship between response and predictors}{10}
\contentsline {paragraph}{Deciding on important variables}{11}
\contentsline {paragraph}{Model fit}{11}
\contentsline {paragraph}{Prediction}{12}
\contentsline {subsection}{\numberline {2.3}Other consideration in the regression model}{12}
\contentsline {paragraph}{Qualitative predictors}{12}
\contentsline {paragraph}{Removing additivity assumption}{13}
\contentsline {paragraph}{Non-linear relationships}{13}
\contentsline {paragraph}{Potential problems}{14}
\contentsline {subparagraph}{Non-linearity of data}{14}
\contentsline {subparagraph}{Correlation of error terms}{14}
\contentsline {subparagraph}{Non-constant variance of error}{14}
\contentsline {subparagraph}{Outliers}{14}
\contentsline {subparagraph}{High leverage points}{15}
\contentsline {subparagraph}{Collinearity}{15}
\contentsline {subsection}{\numberline {2.4}Comparison of linear regression with KNN}{15}
\contentsline {section}{\numberline {3}Chapter 4 - Classification}{17}
\contentsline {subsection}{\numberline {3.1}Logistic regression}{17}
\contentsline {paragraph}{logistic model}{17}
\contentsline {paragraph}{Estimating the regression coefficients}{17}
\contentsline {paragraph}{Making predictions}{18}
\contentsline {paragraph}{Multiple logistic regression}{18}
\contentsline {paragraph}{Logistic regression for more than two response classes}{18}
\contentsline {subsection}{\numberline {3.2}Linear discriminant analysis}{18}
\contentsline {paragraph}{Introduction}{18}
\contentsline {paragraph}{Using Bayes' theorem for classification}{18}
\contentsline {paragraph}{LDA using one predictor p=1}{19}
\contentsline {paragraph}{LDA using multiple predictors p\(>\)1}{20}
\contentsline {subsection}{\numberline {3.3}Quadratic discriminant analysis}{20}
\contentsline {subsection}{\numberline {3.4}A comparison of classification methods}{21}
\contentsline {paragraph}{Logistic regression vs. LDA}{21}
\contentsline {paragraph}{KNN}{21}
\contentsline {paragraph}{QDA}{21}
\contentsline {section}{\numberline {4}Chapter 5 - Resampling Methods}{22}
\contentsline {subsection}{\numberline {4.1}Cross-validation}{22}
\contentsline {paragraph}{Training error rate vs. test error rate}{22}
\contentsline {paragraph}{validation set approach}{22}
\contentsline {paragraph}{Leave-one-out cross-validation (LOOCV)}{22}
\contentsline {paragraph}{k-fold cross-validation}{23}
\contentsline {paragraph}{Bias-variance trade-off for k-fold cross-validation}{24}
\contentsline {subsection}{\numberline {4.2}The bootstrap}{24}
\contentsline {section}{\numberline {5}Chapter 6 - Linear Model Selection and Regularization}{25}
\contentsline {subsection}{\numberline {5.1}Subset selection}{25}
\contentsline {paragraph}{Best subset selection}{25}
\contentsline {paragraph}{Forward stepwise selection}{25}
\contentsline {paragraph}{Backward stepwise selection}{25}
\contentsline {subsection}{\numberline {5.2}Shrinkage methods}{25}
\contentsline {paragraph}{Ridge regression}{25}
\contentsline {paragraph}{The lasso}{26}
\contentsline {paragraph}{Alternative formulations}{26}
\contentsline {paragraph}{Comparing the lasso and ridge regression}{27}
\contentsline {paragraph}{Selecting the tuning parameter}{27}
\contentsline {subsection}{\numberline {5.3}Dimension reduction methods}{27}
\contentsline {paragraph}{Principal components approach}{27}
\contentsline {subparagraph}{Principal components regression}{28}
\contentsline {paragraph}{Partial least squares}{28}
\contentsline {subsection}{\numberline {5.4}Considerations in high dimensions}{28}
\contentsline {paragraph}{High dimensional data}{28}
\contentsline {paragraph}{Interpreting results in high dimensions}{28}
\contentsline {section}{\numberline {6}Chapter 7 - Moving Beyond Linearity}{30}
\contentsline {subsection}{\numberline {6.1}Polynomial regression}{30}
\contentsline {paragraph}{Straightforward extension to linear regression}{30}
\contentsline {subsection}{\numberline {6.2}Step functions}{30}
\contentsline {paragraph}{Allowing for local approximations}{30}
\contentsline {subsection}{\numberline {6.3}Basis functions}{30}
\contentsline {paragraph}{A more general approach}{30}
\contentsline {subsection}{\numberline {6.4}Regression splines}{31}
\contentsline {paragraph}{Piecewise polynomial}{31}
\contentsline {paragraph}{Constraints and splines}{31}
\contentsline {paragraph}{The spline basis representation}{31}
\contentsline {paragraph}{Choosing the number and locations of the knots}{32}
\contentsline {paragraph}{Comparison to polynomial regression}{32}
\contentsline {subsection}{\numberline {6.5}Smoothing splines}{32}
\contentsline {paragraph}{An overview of smoothing splines}{32}
\contentsline {paragraph}{Choosing the smoothing parameter \(\lambda \)}{32}
\contentsline {subsection}{\numberline {6.6}Local regression}{33}
\contentsline {paragraph}{Using local data for prediction}{33}
\contentsline {subsection}{\numberline {6.7}Generalized additive models}{33}
\contentsline {paragraph}{GAMs for regression problems}{33}
\contentsline {paragraph}{Pros and cons of GAMs}{33}
\contentsline {paragraph}{CAMs for classification problems}{34}
\contentsline {section}{\numberline {7}Chapter 8 - Tree-based Methods}{35}
\contentsline {subsection}{\numberline {7.1}The basics of decision trees}{35}
\contentsline {paragraph}{Regression trees}{35}
\contentsline {paragraph}{Prediction via stratification of the feature space}{35}
\contentsline {paragraph}{Tree pruning}{36}
\contentsline {paragraph}{Classification trees}{36}
\contentsline {paragraph}{Trees vs. linear models}{37}
\contentsline {subsection}{\numberline {7.2}Bagging, random forests}{37}
\contentsline {paragraph}{Bagging}{37}
\contentsline {paragraph}{Random forests}{37}
\contentsline {section}{\numberline {8}Chapter 9 - Support Vector Machines}{39}
\contentsline {subsection}{\numberline {8.1}Maximal margin classifier}{39}
\contentsline {paragraph}{What is a hyperplane?}{39}
\contentsline {paragraph}{Classification using a separating hyperplane}{39}
\contentsline {paragraph}{The maximal margin classifier}{39}
\contentsline {paragraph}{The non-separable case}{39}
\contentsline {subsection}{\numberline {8.2}Support vector classifier}{39}
\contentsline {paragraph}{Details of the support vector classifier}{40}
\contentsline {subsection}{\numberline {8.3}Support vector machines}{40}
\contentsline {paragraph}{Classification with non-linear decision boundaries}{40}
\contentsline {paragraph}{Support vector machine (SVM)}{41}
\contentsline {subsection}{\numberline {8.4}SVMs with more than one class}{42}
\contentsline {paragraph}{One-versus-one}{42}
\contentsline {paragraph}{One-versus-all}{42}
\contentsline {subsection}{\numberline {8.5}Relationship to logistic regression}{42}
\contentsline {section}{\numberline {9}Chapter 10 - Unsupervised Learning}{43}
\contentsline {subsection}{\numberline {9.1}Principal Component Analysis}{43}
\contentsline {paragraph}{PCA}{43}
\contentsline {paragraph}{What are principal components?}{43}
\contentsline {paragraph}{Scaling the variables}{44}
\contentsline {paragraph}{Unique - up to a sign}{44}
\contentsline {paragraph}{The proportion of variance}{44}
\contentsline {paragraph}{How many principal components should be used?}{44}
\contentsline {subsection}{\numberline {9.2}Clustering Methods}{44}
\contentsline {paragraph}{Clustering}{44}
\contentsline {paragraph}{K-means clustering}{45}
\contentsline {paragraph}{K-means clustering algorithm}{45}
\contentsline {paragraph}{Hierarchical clustering}{45}
\contentsline {paragraph}{Hierarchical clustering algorithm}{46}
\contentsline {paragraph}{Practical issues}{47}
\contentsline {section}{\numberline {10}Important Formulas}{48}
\contentsline {subsection}{\numberline {10.1}Linear regression}{48}
\contentsline {paragraph}{Linear regression}{48}
\contentsline {paragraph}{Multiple linear regression}{49}
\contentsline {paragraph}{Other considerations}{49}
\contentsline {subsection}{\numberline {10.2}Classification}{50}
\contentsline {paragraph}{Logistic regression}{50}
\contentsline {paragraph}{Linear discriminant analysis}{50}
\contentsline {paragraph}{Quadratic discriminant analysis}{50}
\contentsline {subsection}{\numberline {10.3}Resampling Methods}{51}
\contentsline {paragraph}{Cross validation}{51}
\contentsline {subsection}{\numberline {10.4}Linear Model Selection and Regularization}{51}
\contentsline {paragraph}{Shrinkage methods}{51}
\contentsline {paragraph}{Dimension reduction methods}{51}
\contentsline {subsection}{\numberline {10.5}Moving Beyond Linearity}{52}
\contentsline {paragraph}{Polynomial regression}{52}
\contentsline {paragraph}{Step functions}{52}
\contentsline {paragraph}{Regression splines}{52}
\contentsline {paragraph}{Smoothing splines}{52}
\contentsline {paragraph}{GAM}{52}
\contentsline {subsection}{\numberline {10.6}Tree-based Methods}{53}
\contentsline {paragraph}{The basics of decision trees}{53}
\contentsline {paragraph}{Bagging}{53}
\contentsline {subsection}{\numberline {10.7}Support Vector Machines}{54}
\contentsline {paragraph}{Maximal margin classifier}{54}
\contentsline {paragraph}{Support Vector classifier}{54}
\contentsline {paragraph}{Support Vector machines}{54}
\contentsline {subsection}{\numberline {10.8}Unsupervised Learning}{55}
\contentsline {paragraph}{Principal Component Analysis}{55}
\contentsline {paragraph}{K-means clustering}{55}
\contentsline {paragraph}{Hierarchical clustering}{55}
