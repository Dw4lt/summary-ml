% !TeX root = ../document.tex
\documentclass[../document.tex]{subfiles}
\lstset{inputpath=sections}
\begin{document}

	\subsection{Subset selection}
	So far we have fitted the model to the training data using least squares. We introduce alternative fitting procedures which allows the linear model to perform well in different situation. The goal is to improve the prediction accuracy and the model interpretability.

	\paragraph{Best subset selection}
	This is a brute force approach with \(2^p\) models. The idea is to fit a separate least squares regression to p model having 1 predictor. After that fit a separate least squares regression to p(p-1)/2 models having 2 predictors and so on. We keep from all models with k predictors, the model with the lowest RSS. Among these p+1 models we select the best one using the cross-validated prediction error.

	\paragraph{Forward stepwise selection}
	If p becomes large, the best subset selection is infeasible, since the search space becomes very large. The main difference in this scheme is, that once a predictor makes it into the best model, it will stay there. Hence there are only p-k models to be considered and we pick the model with the best improvement in each step. At the end we use as well the cross-validated prediction error to select the best model out of p+1 models.\\
	Forward stepwise selection is computationally much more efficient but in the other hand, it is not guaranteed to find the best subset possible.\\
	Forward stepwise selection can be applied if n<p, but only models up to order n-1 can be calculated, since the least squares fit does not give a unique solution for p>=n.

	\paragraph{Backward stepwise selection}
	As the name implies, backward stepwise selection starts with a full model of p predictors. In each step k models containing all previous predictors but one are fitted. The best of these models is kept and again at the end the best out of the p+1 model is selected using cross-validation prediction error. The computational complexity is identical to forward stepwise selection and also we might miss the best model but unlike forward stepwise selection, we require that a full model can be fit. Hence \(p<n\).

	\subsection{Shrinkage methods}
	Besides subset selection methods, there are two popular alternative approaches to reduce the variance in the coefficient estimates.

	\paragraph{Ridge regression}
	In ridge regression, the idea is that coefficients that are closer to zero are more desirable. The original optimization problem finds the coefficients such that the RSS is minimized. Now the problem (cost function $J(\beta)$) is modified so that it becomes a trade-off between minimizing the RSS and minimizing the weighted sum of the squared coefficients.
	\begin{equation}
	\begin{split}
		RSS &= \sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij})^2\\
		J(\beta) = RSS+\lambda\sum_{j=1}^{p}\beta_{j}^2 &= \sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij})^2+\lambda\sum_{j=1}^{p}\beta_{j}^2
	\end{split}
	\end{equation}
	The tradeoff factor is called \(\lambda\) and needs to be found separately using cross-validation. In ridge regression, it is important that the predictor variables are standardized.
	\begin{equation}
		\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{ij}-\overline{x}_{j})^2}}
	\end{equation}
	Why does it improve over least squares?\\
	As \(\lambda\) increases the flexibility decreases and this results in higher bias but lower variance. We know there is an optimal \(\lambda\), where the sum of the variance and the \(bias^2\) is minimal. This value will be found using cross-validation. The calculation of the ridge regression is efficient and is almost as fast as the least squares method.

	\paragraph{The lasso}
	Ridge regression does not remove predictors from the model. The lasso approach overcomes this limitation.
	\begin{equation}
	\begin{split}
		J(\beta) = RSS+\lambda\sum_{j=1}^{p}|\beta_{j}| &= \sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_{j}|\\
		\|\beta\|_{1}&=\sum|\beta_{j}|
	\end{split}
	\end{equation}
	The lasso and ridge regression are very similar but the only difference is the norm used in the penalty term. The term also encourages small coefficients as in the ridge regression and in addition, the \(|_{1}\) norm will force some coefficients to exactly zero which results in variable selection. Hence the lasso models are easier to interpret. The lasso can produce a model involving any number of variables and it all depends on the trade-off factor \(\lambda\).

	\paragraph{Alternative formulations}
	It can be shown, that the coefficient estimates solve the following problems:
	\begin{equation}
	\begin{split}
		\text{Ridge minimize }\{\sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij})^2\}\text{ subject to}\sum_{j=1}^{p}\beta_{j}^2\leq s\\
		\text{Lasso minimize }\{\sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij})^2\}\text{ subject to}\sum_{j=1}^{p}|\beta_{j}|\leq s
	\end{split}
	\end{equation}
	For every value of \(\lambda\), there is some s such that the solution to this reformulated problem is the same as to the original problem shown on the right.

	\paragraph{Comparing the lasso and ridge regression}
	Clearly the lasso results in models that are simpler to interpret but neither ridge regression nor the lasso will universally dominate the other. Lasso performs better when only a small number of predictors have a strong relationship with the response and ridge regression performs better when the response is a function of many predictors all with coefficients of roughly equal size. Both approaches are very efficient in calculating the models.

	\paragraph{Selecting the tuning parameter}
	Setting \(\lambda\) selects the resulting model. Hence we use the same approach as in the subsection selection schemes, we use cross-validation to find the best \(\lambda\). The main difference is, that \(\lambda\) is a real number \(\lambda \geq0\) and hence there are infinitely many models. Hence the pragmatic approach is to choose a grid of \(\lambda\) values and compute the cross-validation error.

	\subsection{Dimension reduction methods}
	So far variance reduction was based on subset selection or on shrinking the coefficients. Another class of variance reduction schemes first transforms the original predictors. Let \(Z_{1},Z_{2},...,Z_{m}\) represents \(M<p\) linear combinations of the original predictors.
	\begin{equation}
		Z_{m}=\sum_{j=1}^{p}\phi_{jm}X_{j}
	\end{equation}
	Now we can fit the linear regression model to the transformed data using least squares.
	\begin{equation}
		y_{i}=\theta_{0}+\sum_{m=1}^{M}\theta_{m}z_{im}+\epsilon_{i}, i=1,...,n
	\end{equation}
	Fitting a linear model using least squares in the transform domain can be seen as fitting a linear model in the original domain under the following constraint.
	\begin{equation}
		\beta_{j}=\sum_{m=1}^{M}\theta_{m}\phi_{jm}
	\end{equation}

	\paragraph{Principal components approach}
	PCA is a popular approach of dimensionality reduction in many fields. The first principal component vector defines a line that is as close as possible to the data in that it minimizes the sum of the squared \textbf{perpendicular} distances between each point and the line. The second principal component is also a linear combination of the variables having the largest variance but with the added constraint, that is has to be uncorrelated with the first principal component. This constraint implies that the two principal components need to be orthogonal (perpendicular) to each other. For higher dimensional data (\(p>2\)), more principal components can be calculated. They would successively maximize variance, subject to the constraint of being uncorrelated with the preceding components.
	\subparagraph{Principal components regression}
	First the first M principal components are found and then these are used as predictors in a linear regression model that is fit using least squares. The underlying implicit assumption is, that the directions in which the original predictors show the most variation are the directions that are associated with the response. When using PCR one should work with the standardized predictors so that the units in which the predictors are measured in have no influence.
	\begin{equation}
		\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{ij}-\overline{x}_{j})^2}}
	\end{equation}
	For partial least squares not only the predictors are normalized but also the response, since the response is used to find the optimal transform.

	\paragraph{Partial least squares}
	The PCR approach identifies the directions that best capture the variance in the predictors. This is achieved without using the response, or in other words, this is an unsupervised technique. Hence PCR has a drawback, there is no guarantee that the directions that best describe the predictors will also be the best directions to describe the response. PLS is a supervised alternative to PCR.
	\begin{equation}
	\begin{split}
		Z_{m}&=\sum_{j=1}^{p}\phi_{jm}X_{j}\\
		Z_{1}&=\sum_{j=1}^{p}\phi_{j1}X_{j}
	\end{split}
	\end{equation}
	First the predictors and the response are standardized. Now the first direction score \(Z_{1}\) is computed by setting the \(\phi_{j1}\)'s equal to the coefficients from the simple linear regression of Y onto the \(X_{j}\)'s\\
	Each of the predictor variable is adjusted for \(Z_{1}\) by regressing each variable on \(Z_{1}\) and taking residuals. Now \(Z_{2}\) can be computed using this orthogonalized data in exactly the same way as \(Z_{1}\) was computed using the original data. This iterative approach can be repeated M times to identify the M PLS components \(Z_{1},...,Z_{M}\). Finally a linear model predicting the response Y using \(Z_{1},...,Z_{M}\) is fitted using leas squares. Again, M is a tuning parameter which is usually chosen by cross-validation.

	\subsection{Considerations in high dimensions}

	\paragraph{High dimensional data}
	Traditionally, the number of predictors was much smaller than the number of observations. The situation is changing with the ease that data can be collected today.

	\paragraph{Interpreting results in high dimensions}
	Multi-collinearity is a major problem in high dimensional data. If \(p>n\) then any predictor variable can be written as a linear combination of the other variables. Hence we can never know exactly which variables truly are predictive of the outcome and we can never identify the best coefficients for use in the regression. Also error reporting on the training set is problematic in high dimensions. If the number of predictors rises (\(p>n\)), even when they are completely unrelated to the response, the training MSE and the \(R^2\) value can be made arbitrarily good. Hence it is fundamental to report results on an independent test set, or fi this is not possible, perform cross-validation.
\end{document}
