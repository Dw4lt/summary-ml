% !TeX root = ../document.tex
\documentclass[../document.tex]{subfiles}
\lstset{inputpath=sections}
\begin{document}

	\subsection{Linear regression}

	\paragraph{Linear regression}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|ccc|}{|ccccccccccccc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $Y \approx \beta_{0} + \beta_{1}X$ &\\
			$\hat{\beta_{1}}$ &
			$\hat{\beta}_{1}=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}$&\\
			$\hat{\beta_{0}}$ & $\hat{\beta}_{0}=\overline{y}-\hat{\beta}_{1}\overline{x}$ &\\
			True variance & $Var(\hat{\mu})=SE(\hat{\mu})^2=\frac{\sigma^2}{n}$ &\\
			 & $SE(\hat{\beta}_{0})^2=\sigma^2(\frac{1}{n}+\frac{\overline{x}^2}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2})$ &\\
			 & $SE(\hat{\beta}_{1})^2=\frac{\sigma^2}{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}$ &\\
			Estimated variance& $\sqrt{\frac{RSS}{n-2}} = \sqrt{\frac{\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^2}{n-2}}$ &\\
			 & $\hat{\beta}_{1}\pm 2*SE(\hat{\beta}_{1})$&\\
			 & $\hat{\beta}_{0}\pm 2*SE(\hat{\beta}_{0})$&\\
			t-statistic& $t = \frac{\hat{\beta}_{1}-0}{SE(\hat{\beta}_{1})}$ &reject \(H_{0}\) t\(>\)2(5\%) t\(>\)2.75(1\%)\\
			Accuracy&$TSS = \sum(y_{i}-\overline{y})^2$&\\
			 & $R^2 = \frac{TSS - RSS}{TSS} = 1-\frac{RSS}{TSS}$& $0\leq R^2\leq 1, \approx$ 1, much explained\\
			 & $Cor(X,Y)=\frac{\sum_{i=1}^{n}(x_{i}-\overline{x})(y_{i}-\overline{y})}{\sqrt{\sum_{i=1}^{n}(x_{i}-\overline{x})^2}\sqrt{\sum_{i=1}^{n}(y_{i}-\overline{y})^2}}$ &\\
		\end{TAB}
	\end{center}
	\sectionbreak

	\paragraph{Multiple linear regression}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|ccc|}{|ccc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $Y = \beta_{0}+\beta_{1}X_{1}++\beta_{2}X_{2}+...+\beta_{n}X_{n}+\epsilon$ & \\
			RSS &
			$RSS = \sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^2= \sum_{i=1}^{n}(y_{i}-\hat{\beta}_{0}-\hat{\beta}_{1}x_{i1}-...-\hat{\beta}_{n}x_{in})^2$ & \\
			F-statistic & $F = \frac{\frac{TSS-RSS}{p}}{\frac{RSS}{n-p-1}}$ & F\(>\)1 good model\\
		\end{TAB}
	\end{center}

	\paragraph{Other considerations}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|ccc|}{|cccccc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Qualitative predictors & $y_{i} =\begin{cases}
			\beta_{0}+\beta_{1}+\epsilon_{i} &\text{if ith person is x}\\
			\beta_{0}+\beta_{2}+\epsilon_{i} &\text{if ith person is y}\\
			\beta_{0}+\epsilon_{i} &\text{if ith person is African z}\\
			\end{cases}$ &\\
			Removing additivity assumption &
			$\begin{cases}
			(\beta_{0}+\beta_{2})+(\beta_{1}+\beta_{3})X_{1} &\text{if a}\\
			\beta_{0}+\beta_{1}X_{1} &\text{if not a}
			\end{cases}$&\\
			Non-linear & $Y = \beta_{0}+\beta_{1}x+\beta_{2}x^2+\epsilon$& \\
			Outliers & $\frac{\hat{\epsilon}_{i}}{\sigma\sqrt{1-h_{i}}} $&$>3=\text{outlier}$\\
			High leverage points &$h_{i} = \frac{1}{n}+\frac{(x_{i}-\overline{x})^2}{\sum_{i'=1}^{n}(x_{i'}-\overline{x})^2} $&$>\frac{p+1}{n}=\text{hl point}$\\
			Collinearity & $VIF(\hat{\beta}_{j})=\frac{1}{1-R^2_{X_{j}|X_{-j}}}$&$>5-10=\text{problem}$\\
		\end{TAB}
	\end{center}
	\sectionbreak

	\subsection{Classification}

	\paragraph{Bayes}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|c|}{|cc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			$P(X|Y)=\frac{P(X)P(Y|X)}{P(Y)}$ \\
			$P(X)=\frac{P(Y)P(X|Y)}{P(Y|X)}$
		\end{TAB}
	\end{center}

	\paragraph{Logistic regression}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|ccc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $p(X)=\frac{e^{\beta_{0}+\beta_{1}X}}{1+e^{\beta_{0}+\beta_{1}X}}$ \\
			Coefficients & $l(\beta_{0},\beta_{1})=\prod_{i:y_{i}=1}p(x_{i})\prod_{i':y_{i'}=0}(1-p(x_{i'}))$ \\
			Multiple LR & $p(X)=\frac{e^{\beta_{0}+\beta_{1}X_{1}+...+\beta_{p}X_{p}}}{1+e^{\beta_{0}+\beta_{1}X_{1}+...+\beta_{p}X_{p}}}$ \\
		\end{TAB}
	\end{center}

	\paragraph{Linear discriminant analysis}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|ccccc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $\hat{\delta}_{k}(x)=x*\frac{\hat{\mu}_{k}}{\hat{\sigma}^2}-\frac{\hat{\mu}_{k}^2}{2\hat{\sigma}^2}+log(\hat{\pi}_{k})$ \\
			$\mu$ & $\hat{\mu_{k}}=\frac{1}{n_{k}}\sum_{i:y_{i}=k}x_{i}$ \\
			$\sigma$ & $\hat{\sigma^2}=\frac{1}{n-K}\sum_{k=1}^{K}\sum_{i:y_{i}=k}(x_{i}-\hat{\mu_{k}})^2$\\
			frequency & $\hat{\pi_{k}}=n_{k}/n$\\
			multiple predictors & $\delta_{k}(x)=x^T\Sigma^{-1}\mu_{k}-\frac{1}{2}\mu_{k}^T\Sigma^{-1}\mu_{k}+log\pi_{k}$\\
		\end{TAB}
	\end{center}

	\paragraph{Quadratic discriminant analysis}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|c|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $\delta_{k}(x)=-\frac{1}{2}(x-\mu_{k})^T\Sigma_{k}^-1(x-\mu_{k})-\frac{1}{2}log|\Sigma_{k}|+log\pi_{k}$ \\
		\end{TAB}
	\end{center}
	\sectionbreak

	\subsection{Resampling Methods}

	\paragraph{Cross validation}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|cc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			LOOCV & $CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}MSE_{i}$ \\
			k-fold & $CV_{(k)}=\frac{1}{k}\sum_{i=1}^{k}MSE_{i}$\\
		\end{TAB}
	\end{center}

	\subsection{Linear Model Selection and Regularization}

	\paragraph{Shrinkage methods}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|ccc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Ridge Regression & $RSS+\lambda\sum_{j=1}^{p}\beta_{j}^2 = \sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij})^2+\lambda\sum_{j=1}^{p}\beta_{j}^2$ \\
			Standardization RR & $\tilde{x}_{ij}=\frac{x_{ij}}{\sqrt{\frac{1}{n}\sum_{i=1}^{n}(x_{ij}-\overline{x}_{j})^2}}$\\
			Lasso & $	RSS+\lambda\sum_{j=1}^{p}|\beta_{j}| = \sum_{i=1}^{n}(y_{i}-\beta_{0}-\sum_{j=1}^{p}\beta_{j}x_{ij})^2+\lambda\sum_{j=1}^{p}|\beta_{j}|$\\
		\end{TAB}
	\end{center}

	\paragraph{Dimension reduction methods}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|c|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form (linear) & $y_{i}=\theta_{0}+\sum_{m=1}^{M}\theta_{m}z_{im}+\epsilon_{i}, i=1,...,n$ \\
		\end{TAB}
	\end{center}
	\sectionbreak

	\subsection{Moving Beyond Linearity}

	\paragraph{Polynomial regression}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|c|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^2+\beta_{3}x_{i}^3+...+\beta_{d}x_{i}^d+\epsilon_{i}$ \\
		\end{TAB}
	\end{center}

	\paragraph{Step functions}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|c|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $y_{i}=\beta_{0}+\beta_{1}C_{1}(x_{i})+\beta_{2}C_{2}(x_{i})+...+\beta_{K}C_{K}(x_{i})+\epsilon_{i}$ \\
		\end{TAB}
	\end{center}

	\paragraph{Regression splines}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|cc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^2+\beta_{3}x_{i}^3+\beta_{4}h(x,\xi)+\epsilon_{i}$ \\
			truncated power basis & $h(x,\xi)=(x-\xi)_{+}^3=
			\begin{cases}
			(x-\xi)^3 \text{  if  } x > \xi\\
			0 \text{  otherwise  }
			\end{cases}$ \\
		\end{TAB}
	\end{center}

	\paragraph{Smoothing splines}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|cc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $\sum_{i=1}^{n}(y_{i}-g(x_{i}))^2+\lambda\int g''(t)^2dt$ \\
			Choosing $\lambda$ & $CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}(\frac{y_{i}-\hat{y}_{i}}{1-h_{i}})^2$ \\
		\end{TAB}
	\end{center}

	\paragraph{GAM}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|c|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $y_{i}=\beta_{0}+\sum_{j=1}^{p}f_{j}(x_{ij})+\epsilon_{i} = \beta_{0}+f_{1(}x_{i1})+f_{2}(x_{i2})+...+f_{p}(x_{ip})+\epsilon_{i}$ \\
		\end{TAB}
	\end{center}
	\sectionbreak

	\subsection{Tree-based Methods}

	\paragraph{The basics of decision trees}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|cccccc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			B & Number of trees voting on result \\
			Regression trees & $RSS = \sum_{j=1}^{J}\sum_{i\in R_{j}}(y_{i}-\hat{y}_{R_{j}})^2$ \\
			Tree pruning & $\sum_{m=1}^{|T|}\sum_{x_{i}} \in R_{m}(y_{i}-\hat{y}_{R_{m}})^2 + \alpha|T|$\\
			Classification error index & $E = 1-max(\hat{p}_{mk})$\\
			Gini index & $G = \sum_{k=1}^{K}\hat{p}_{mk}(1-\hat{p}_{mk})$ \\
			Cross-entropy & $D = -E\left[log(\hat{p}_{mk})\right] = -\sum_{k=1}^{K}\hat{p}_{mk}log(\hat{p}_{mk})$\\
		\end{TAB}
	\end{center}

	\paragraph{Bagging}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|c|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Bagging & $\hat{f}_{bag}(x)=\frac{1}{B}\sum_{b=1}^{B}\hat{f}^{*b}(x)$\\
		\end{TAB}
	\end{center}
	\sectionbreak

	\subsection{Support Vector Machines}

	\paragraph{Maximal margin classifier}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|c|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $f(x^*)=\beta_{0}+\beta_{1}x_{1}^*+\beta_{2}x_{2}^*+...+\beta_{p}x_{p}^*$ \\
		\end{TAB}
	\end{center}

	\paragraph{Support Vector classifier}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|cc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $\text{maximize M}\text{subject to}\sum_{j=1}^{p}\beta_{j}^2=1$\\
			Slackvariable & $\epsilon_{i}\ge 0, \sum_{i=1}^{n}\epsilon_{i}\ge C$\\
		\end{TAB}
	\end{center}

	\paragraph{Support Vector machines}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|ccc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $f(x)=\beta_{0}+\sum_{i=1}^{n}\alpha_{i}(x,x_{i})$\\
			Kernel & $K(x_{i},x_{i'})=\sum_{j=1}^{p}x_{ij}x_{i'j}$\\
			Polynomial Kernel & $K(x_{i},x_{i'})=(1+\sum_{j=1}^{p}x_{ij}x_{i'j})^d$\\
		\end{TAB}
	\end{center}
	\sectionbreak

	\subsection{Unsupervised Learning}

	\paragraph{Principal Component Analysis}
	\begin{center}
		\begin{TAB}(@,0.5cm,1cm)[8pt]{|cc|}{|cc|}% (rows,min,max)[tabcolsep]{columns}{rows}
			Form & $Z_{1}=\phi_{11}X_{1}+\phi_{21}X_{2}+...+\phi_{p1}X_{p}$ \\
			PVE & $\frac{\sum_{i=1}^{n}(\sum_{j=1}^{p}\phi_{jm}x_{ij})^2}{\sum_{j=1}^{p}\sum_{i=1}^{n}x_{ij}^2}$\\
		\end{TAB}
	\end{center}

	\paragraph{K-means clustering}
	\begin{center}
		\begin{enumerate}
			\item Randomly assign a number, from 1 to K, to each of the observations. These serve as initial cluster assignment for the observations.
			\item Iterate until the cluster assignments stop changing:
			\begin{itemize}
				\item For each of the K clusters, compute the cluster centroid. The kth cluster centroid is the vector of the p feature means for the observations in the kth cluster.
				\item Assign each observation to the cluster whose centroid is closest (Euclidean distance)
			\end{itemize}
		\end{enumerate}
	\end{center}

	\paragraph{Hierarchical clustering}
	\begin{center}
		\begin{enumerate}
			\item Begin with n observations and a measure (Euclidean distance) of all the
			\(
			\begin{pmatrix}
			n\\2
			\end{pmatrix} = n(n-1)/2
			\)
			pairwise dissimilarities. Treat each observation as its own cluster.
			\item For i = n, n-1,...,2:
			\begin{itemize}
				\item Examine all pairwise inter-cluster dissimilarities among the i clusters and identify the pair of clustersthat are least dissimilar (that is, most similar). Fuse these two clusters. The dissimilarity between these two clusters indicates the height in the dendrogram at which the fusion should be placed.
				\item Compute the new pairwise inter-cluster dissimilarities among the i-1 remaining clusters.
			\end{itemize}
		\end{enumerate}
	\end{center}

\end{document}
