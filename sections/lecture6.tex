% !TeX root = ../document.tex
\documentclass[../document.tex]{subfiles}
\lstset{inputpath=sections}
\begin{document}	
	\subsection{Polynomial regression}
	Linear models used so far are easy to interpret and if the true relationship is not approximately linear, limited in predictive power. So we increase the predictive power but want to maintaining as much interpretability as possible if we relax the linearity assumption.
	\paragraph{Straightforward extension to linear regression}
	We admit different powers of the original predictor variables as new predictor variables and then perform linear regression.
	\begin{equation}
		y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^2+\beta_{3}x_{i}^3+...+\beta_{d}x_{i}^d+\epsilon_{i}
	\end{equation}
	We can also use the same model as we used for linear regression for classification problems.
	\begin{equation}
		Pr(y_{i}>250|x_{i})=\frac{exp(\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^2+...+\beta_{d}x_{i}^d)}{1+exp(\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^2+...+\beta_{d}x_{i}^d)}
	\end{equation}
	\subsection{Step functions}
	\paragraph{Allowing for local approximations}
	Step functions allow for local approximations by braking X into bins and fitting a different constant in each bin.
	\begin{equation}
	\begin{split}
		y_{i}=\beta_{0}+\beta_{1}C_{1}(x_{i})+\beta_{2}C_{2}(x_{i})+...+\beta_{K}C_{K}(x_{i})+\epsilon_{i}\\
		C_{0}(X)+C_{1}(X)+...+C_{K}(X)=1
	\end{split}
	\end{equation}
	It's basically again a linear functions. So we can also use this approach to classification problems.
	\begin{equation}
		Pr(y_{i}>250|x_{i})=\frac{exp(\beta_{0}+\beta_{1}C_{1}(x_{i})+\beta_{2}C_{2}(x_{i})+...+\beta_{K}C_{K}(x_{i}))}{1+exp(\beta_{0}+\beta_{1}C_{1}(x_{i})+\beta_{2}C_{2}(x_{i})+...+\beta_{K}C_{K}(x_{i}))}
	\end{equation}
	\subsection{Basis functions}
	\paragraph{A more general approach}
	Polynomial and piecewise-constant regression models are special cases of a basis function approach. Instead of fitting a linear model in X we fit a linear model in some basis functions \(b_{j}\)
	\begin{equation}
	\begin{split}
		&y_{i}=\beta_{0}+\beta_{1}b_{1}(x_{i})+\beta_{2}b_{2}(x_{i})+...+\beta_{K}b_{K}(x_{i})+\epsilon_{i}\\
		&\text{For polynomial regression:}\\
		&b_{j}(x_{i})=x_{i}^j\\
		&b_{j}(x_{i})=I(c_{j}\leq x_{i} < c_{j+1})
	\end{split}
	\end{equation}
	This is an elegant approach. We can use standard linear regression but applying it to the model having the original predictor values transformed by the basis functions. Hence all the inference tools for linear models are available for this non-linear fitting method.
	\subsection{Regression splines}
	\paragraph{Piecewise polynomial}
	Combining the local approach of step function and the flexibility of polynomials by fitting low degree polynomials over different regions of X.\\
	For example a cubic polynomial of the following form is fitted locally.
	\begin{equation}
	\begin{split}
		y_{i}&=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^2+\beta_{3}x_{i}^3+\epsilon_{i}\\
		y_{i}&=\begin{cases}
			\beta_{01}+\beta_{11}x_{i}+\beta_{21}x_{i}^2+\beta_{31}x_{i}^3+\epsilon_{i} \text{  if  } x_{i} < c\\
			\beta_{02}+\beta_{12}x_{i}+\beta_{22}x_{i}^2+\beta_{32}x_{i}^3+\epsilon_{i} \text{  if  } x_{i} \geq c
		\end{cases}
	\end{split}
	\end{equation}
	For example if there is a knot at c we have a cubic polynomial on the left and another one the right of c. Using more knots leads to more flexible fits.
	\paragraph{Constraints and splines}
	We would like the overall function to be continous. Hence we add the constraint, that at a knot both polynomials must have the same values. We also want the first and the second derivatives to be continuous. This will result in a very smooth transition from one polynomial to the next. Hence two more constraints need to be added per knot. Each constraint removes a degree of freedom.
	\paragraph{The spline basis representation}
	How can we fit cubic splines to the data? The most elegant approach is based on the spline basis representation. 
	\begin{equation}
		y_{i}=\beta_{0}+\beta_{1}b_{1}(x_{i})+\beta_{2}b_{2}(x_{i})+...+\beta_{K+3}b_{K+3}(x_{i})+\epsilon_{i}
	\end{equation}
	We use the truncated power basis function, where \(\xi\) is the knot.
	\begin{equation}
		h(x,\xi)=(x-\xi)_{+}^3=
		\begin{cases}
			(x-\xi)^3 \text{  if  } x > \xi\\
			0 \text{  otherwise  }
		\end{cases}
	\end{equation}
	One can show, that the following expression will have a discontinuity in only the third derivatevie at \(\xi\). In other words, the function will stay continuous and have continuous first and second derivatives.
	\begin{equation}
		y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^2+\beta_{3}x_{i}^3+\beta_{4}h(x,\xi)+\epsilon_{i}
	\end{equation}
	Hence in order to fit a cubic spline to a data set using K knots we perform least squares regression with an intercept and 3+K predictors of the following form.
	\begin{equation}
		X, X^2, X^3, h(X,\xi_{1}),h(X,\xi_{2}),...,h(X,\xi_{K})
	\end{equation}
	Unfortunately splines can have high variance at the outer range of the predictors and a natural spline is a regression spline with additional boundary constraints.
	\paragraph{Choosing the number and locations of the knots}
	Clearly more knots increase the flexibility. Also one might want to place the knots such that many knots are where much flexibility is necessary and few knots are where less flexibility is required. In practice this is hard to know a priori and hence the knots are uniformly spaced (\(25^th,50^th and 75^th percentiles\))\\
	We use cross-validation to determine the optimal number of knots.
	\paragraph{Comparison to polynomial regression}
	Regression splines are usually superior to polynomial regression. Regression splines can increase flexibility by adding knots, while polynomial regression needs to add powers of the predictor variable.
	
	\subsection{Smoothing splines}
	\paragraph{An overview of smoothing splines}
	The problem of fitting a function to the data can be solved by minimizing the RSS. On way of measuring toughness is by integrating the square of the second derivative.
	\begin{equation}
	\begin{split}
		RSS=\sum_{i=1}^{n}(y_{i}-g(x_{i}))^2\\
		\int g''(t)^2dt
	\end{split}
	\end{equation}
	Hence we want to fit a function to the data that minimizes the RSS and the weighted roughness measure.
	\begin{equation}
		\sum_{i=1}^{n}(y_{i}-g(x_{i}))^2+\lambda\int g''(t)^2dt
	\end{equation}
	Again there is a trade-off factor \(\lambda\) involved which is nonnegative and is a tuning parameter.
	\paragraph{Choosing the smoothing parameter \(\lambda\)}
	A smoothing spline is a natural cubic spline with knots at every unique value of \(x_{i}\). This implies that this spline has far too many degrees of freedom. This is where the smoothing parameter comes into play, since it controls the roughness of the smoothing spline it controls the effective degrees of freedom. The effective degrees of freedom measure the flexibility of the spline, the higher it is, the more flexible the spline is. Hence we do not need to pick the locations of the knots as before, but now we have to select the best smoothing parameter. As always in such situation, we use cross-validation to find the \(\lambda\) which results in the smallest cross-validated RSS.
	\begin{equation}
		CV_{(n)}=\frac{1}{n}\sum_{i=1}^{n}(\frac{y_{i}-\hat{y}_{i}}{1-h_{i}})^2
	\end{equation}
	\subsection{Local regression}
	\paragraph{Using local data for prediction}
	Local regression uses only data close to a target point \(x_{0}\) to fit a flexible non-linear function. Local regression has several parameters:
	\begin{itemize}
		\item Weighting function \(K(x_{i},x_{0})\)
		\item degree of the polynomial fit
		\item span s which defines how many training points are used for the local fit
	\end{itemize}
	The most important choice is the span s. The smaller the vlaue of s, the more local the fit is and hence the more flexible the function becomes. Again, cross-validation is used for finding the best vlaue for the span s.
	\subsection{Generalized additive models}
	All methods of this chapter have been developed for a single predictor. Gneralized additive models (GAMs) extends these ideas to multiple predicotors.
	\paragraph{GAMs for regression problems}
	Recall the multiple regression model.
	\begin{equation}
		y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+...+\beta_{p}x_{ip}+\epsilon_{i}
	\end{equation}
	The obvious extension of this formula is by replacing the linear components \(\beta_{j}x_{ij}\) with a smooth non-linear function \(f_{j}(x_{ij})\), resulting in the new model.
	\begin{equation}
		y_{i}=\beta_{0}+\sum_{j=1}^{p}f_{j}(x_{ij})+\epsilon_{i} = \beta_{0}+f_{1(}x_{i1})+f_{2}(x_{i2})+...+f_{p}(x_{ip})+\epsilon_{i}
	\end{equation}
	This is an example of a GAM. It is called an additive model because we calculate a separate \(f_{j}\) for each \(X_{j}\) and then add together all of their contributions. 
	\paragraph{Pros and cons of GAMs}
	Pros of GAMs
	\begin{itemize}
		\item GAMs allow us to fit a non-linear \(f_{j}\) for each \(X_{j}\), so that we can automatically model non-linear relationships that standard linear regression will miss. This means that we do not need to manually try out many different transformations on each variable individually. 
		\item The non-linear fits can potentially make more accurate predictions for the response Y.
		\item Because the model is additive, we can still examine the effect of each \(X_{j}\) on > individually while holding all of the other variables fixed. Hence if we are interested in inference, GAMs provide a useful representation.
	\end{itemize}
	The main negative aspect is that the model is restricted to be additive. With many variables, important interactions can be missed. However, as with linear regression, we can manually add interaction terms to the GAM model.
	\paragraph{CAMs for classification problems}
	GAMs are also useful, if the responses are qualitative. 
	\begin{equation}
		log(\frac{p(X)}{1-p(X)})=\beta_{0}+f_{1}(X_{1})+f_{2}(X_{2})+...+f_{p}(X_{p})
	\end{equation}
	For simplicity, we assume the response is either 1 or 0.
\end{document}